# Requirements

```
pip install -r requirements.txt
```

# Natural Questions Multi-Agent System

This repository contains a multi-agent system for answering questions from the Natural Questions dataset. The system consists of three main components:

## Data Extraction (`data_extraction.py`)

Utilities for processing and sampling from the Natural Questions dataset:

Core:

- `sample_short_ans_examples()`: Sample K examples that have short answers, and write to output JSONL file + return the list
- `count_lines()`: Count number of lines in large files
- `read_first_lines()`: Read first N lines from a file

Extra:

- `sample_examples()`: Randomly sample K examples from a JSONL file
- `keep_short_ans_examples()`: Filter examples to only keep those with short answers
- `write_jsonl()`: Write examples to JSONL format

Example usage:
```python
# Sample 10 examples with short answers only
DEV_FILE = '.local_data/v1.0-simplified_nq-dev-all.jsonl'

# side effect: automatically write to file
temp = data_extraction.sample_short_ans_examples(DEV_FILE,k=10,seed=42) 
```


## Multi-Agent System (`multi_agent.py`) 

This is a template class, BaseAgentSystem, for a multi-agent system. The following helpers are implemented:

- `predict_batch()`: Make predictions for a batch of examples, transform the output into eval-ready format
- `_find_seq_index()`: Find the start and end token indices of a sequence in a document
- `format_prediction()`: Format the prediction into the format of Natural Questions evaluation

To use this template, You need to wrapper your agent system in self.predict() method.
```python
self.predict(example: Dict, verbose: bool) -> pred_str: str, score: float
```

As an example(in the file `multi_agent.py`), a three-agent system for extracting answers from Natural Questions examples:
1. First agent extracts initial answer from context
2. Second agent cuts answer to shortest relevant substring
3. Third agent refines and improves the answer


Example usage:
```python
from nq_agents import multi_agent
from nq_agents import data_extraction

# Initialize the multi-agent system
my_agents = multi_agent.NQMultiAgent()

# Read input examples
input_examples = data_extraction.read_first_lines('data/two-dev.jsonl')

# Make predictions. side effect: write to file
predictions = my_agents.predict_batch(input_examples, verbose=True)
```

## Evaluation (`evaluation.py`)

Evaluation utilities for assessing model performance:

- `evaluate_predictions()`: Evaluate predictions against gold examples
- Integration with Google's official NQ evaluation metrics
- Support for both short and long answer evaluation
- F1 score calculation

Example usage:
```python
from nq_agents import evaluation

# Evaluate predictions
GOLD_FILE = 'data/two-dev.jsonl'
PREDICTIONS_FILE = 'predictions_None_20241109_025918.jsonl' # generated by multi_agent.py

result = evaluation.evaluate_predictions(GOLD_FILE, PREDICTIONS_FILE)
```